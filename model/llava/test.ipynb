{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/LLaSA\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('../..')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/llasa/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='linear': {'type'}\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:08<00:00,  2.98s/it]\n",
      "Some weights of LLaSA were not initialized from the model checkpoint at /workspace/LLaSA/checkpoints/LLaVA-NeXT-Video-7B-hf and are newly initialized: ['seg_projector.linear.bias', 'seg_projector.linear.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# reload module \n",
    "import importlib\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig, LlavaNextVideoForConditionalGeneration\n",
    "from model.llasa_arch import LLaSA\n",
    "from model.llasa_processor import LlavaNextVideoProcessor\n",
    "\n",
    "\n",
    "importlib.reload(importlib.import_module(\"model.llasa_arch\"))\n",
    "importlib.reload(importlib.import_module(\"model.llasa_processor\"))\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "#del processor\n",
    "#del model\n",
    "\n",
    "processor = LlavaNextVideoProcessor.from_pretrained(\"/workspace/LLaSA/checkpoints/LLaVA-NeXT-Video-7B-hf\", cache_dir='/workspace/LLaSA/checkpoints')\n",
    "model = LLaSA.from_pretrained(\n",
    "    \"/workspace/LLaSA/checkpoints/LLaVA-NeXT-Video-7B-hf\",\n",
    "    quantization_config=quantization_config,\n",
    "    device_map='auto',\n",
    "    cache_dir = '/workspace/LLaSA/checkpoints'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import av\n",
    "import numpy as np\n",
    "\n",
    "def read_video_pyav(container, indices):\n",
    "    '''\n",
    "    Decode the video with PyAV decoder.\n",
    "\n",
    "    Args:\n",
    "        container (av.container.input.InputContainer): PyAV container.\n",
    "        indices (List[int]): List of frame indices to decode.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: np array of decoded frames of shape (num_frames, height, width, 3).\n",
    "    '''\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            frames.append(frame)\n",
    "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncontainer = av.open(video_path_2)\\n\\n# sample uniformly 8 frames from the video (we can sample more for longer videos)\\ntotal_frames = container.streams.video[0].frames\\nindices = np.arange(0, total_frames, total_frames / 8).astype(int)\\nclip_karate = read_video_pyav(container, indices)'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Download video from the hub\n",
    "#video_path_1 = hf_hub_download(repo_id=\"raushan-testing-hf/videos-test\", filename=\"sample_demo_1.mp4\", repo_type=\"dataset\")\n",
    "#ideo_path_2 = hf_hub_download(repo_id=\"raushan-testing-hf/videos-test\", filename=\"karate.mp4\", repo_type=\"dataset\")\n",
    "\n",
    "video_path_1 = '/workspace/LLaSA/dataset/A2D/clips320H/__KkKB4wzrY.mp4'\n",
    "\n",
    "container = av.open(video_path_1)\n",
    "\n",
    "# sample uniformly 8 frames from the video (we can sample more for longer videos)\n",
    "total_frames = container.streams.video[0].frames\n",
    "indices = np.arange(0, total_frames, total_frames / 8).astype(int)\n",
    "clip_baby = read_video_pyav(container, indices)\n",
    "\n",
    "\"\"\"\n",
    "container = av.open(video_path_2)\n",
    "\n",
    "# sample uniformly 8 frames from the video (we can sample more for longer videos)\n",
    "total_frames = container.streams.video[0].frames\n",
    "indices = np.arange(0, total_frames, total_frames / 8).astype(int)\n",
    "clip_karate = read_video_pyav(container, indices)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_baby.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "# np array with shape (frames, height, width, channels)\n",
    "video = clip_baby\n",
    "\n",
    "fig = plt.figure()\n",
    "im = plt.imshow(video[0,:,:,:])\n",
    "\n",
    "plt.close() # this is required to not display the generated image\n",
    "\n",
    "def init():\n",
    "    im.set_data(video[0,:,:,:])\n",
    "\n",
    "def animate(i):\n",
    "    im.set_data(video[i,:,:,:])\n",
    "    return im\n",
    "\n",
    "#anim = animation.FuncAnimation(fig, animate, init_func=init, frames=video.shape[0],\n",
    "#                               interval=100)\n",
    "#HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets also load 2 images for generation from image data\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "image_stop = Image.open(requests.get(\"https://www.ilankelman.org/stopsigns/australia.jpg\", stream=True).raw)\n",
    "image_snowman = Image.open(requests.get(\"https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/snowman.jpg\", stream=True).raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each \"content\" is a list of dicts and you can add image/video/text modalities\n",
    "conversation = [\n",
    "      {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": [\n",
    "              {\"type\": \"text\", \"text\": \"Why is this video funny?\"},\n",
    "              {\"type\": \"video\"},\n",
    "              {\"type\": \"seg\"},\n",
    "              ],\n",
    "      },\n",
    "]\n",
    "\n",
    "conversation_2 = [\n",
    "      {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": [\n",
    "              {\"type\": \"text\", \"text\": \"What do you see in this video?\"},\n",
    "              {\"type\": \"video\"},\n",
    "              ],\n",
    "      },\n",
    "]\n",
    "\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "#prompt_2 = processor.apply_chat_template(conversation_2, add_generation_prompt=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor([prompt, prompt_2], videos=[clip_baby, clip_karate], padding=True, return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 4096])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "seg = torch.ones(1, 4096, 256)\n",
    "seg.shape\n",
    "seg = seg.transpose(1,2)\n",
    "print(seg.shape)\n",
    "seg = seg.to(torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_baby.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'USER: <video>\\nWhy is this video funny? <seg>\\nASSISTANT:'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs = processor([prompt], videos=[clip_baby], padding=True, return_tensors=\"pt\").to(model.device)\n",
    "inputs = processor([prompt], videos=[clip_baby], seg=seg, labels='sda',  padding=True, return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   1,  269, 1388]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1]], device='cuda:0'), 'pixel_values_videos': tensor([[[[[ 9.9604e-01,  9.9604e-01,  9.9604e-01,  ...,  3.8290e-01,\n",
       "             3.8290e-01,  3.8290e-01],\n",
       "           [ 1.0106e+00,  1.0106e+00,  1.0106e+00,  ...,  3.8290e-01,\n",
       "             3.8290e-01,  3.8290e-01],\n",
       "           [ 1.0106e+00,  1.0106e+00,  1.0106e+00,  ...,  3.8290e-01,\n",
       "             3.8290e-01,  3.8290e-01],\n",
       "           ...,\n",
       "           [-1.6901e+00, -1.6901e+00, -1.6901e+00,  ..., -1.7923e+00,\n",
       "            -1.7923e+00, -1.7923e+00],\n",
       "           [-1.6901e+00, -1.6901e+00, -1.6901e+00,  ..., -1.7923e+00,\n",
       "            -1.7923e+00, -1.7923e+00],\n",
       "           [-1.6901e+00, -1.6901e+00, -1.6901e+00,  ..., -1.7923e+00,\n",
       "            -1.7923e+00, -1.7923e+00]],\n",
       "\n",
       "          [[ 1.1444e+00,  1.1444e+00,  1.1444e+00,  ...,  5.5910e-01,\n",
       "             5.5910e-01,  5.5910e-01],\n",
       "           [ 1.1594e+00,  1.1594e+00,  1.1594e+00,  ...,  5.5910e-01,\n",
       "             5.5910e-01,  5.5910e-01],\n",
       "           [ 1.1594e+00,  1.1594e+00,  1.1594e+00,  ...,  5.5910e-01,\n",
       "             5.5910e-01,  5.5910e-01],\n",
       "           ...,\n",
       "           [-1.5570e+00, -1.5570e+00, -1.5570e+00,  ..., -1.6921e+00,\n",
       "            -1.6921e+00, -1.6921e+00],\n",
       "           [-1.5570e+00, -1.5570e+00, -1.5570e+00,  ..., -1.6921e+00,\n",
       "            -1.6921e+00, -1.6921e+00],\n",
       "           [-1.5570e+00, -1.5570e+00, -1.5570e+00,  ..., -1.6921e+00,\n",
       "            -1.6921e+00, -1.6921e+00]],\n",
       "\n",
       "          [[ 1.2643e+00,  1.2643e+00,  1.2643e+00,  ...,  7.9499e-01,\n",
       "             7.9499e-01,  7.9499e-01],\n",
       "           [ 1.2785e+00,  1.2785e+00,  1.2785e+00,  ...,  7.9499e-01,\n",
       "             7.9499e-01,  7.9499e-01],\n",
       "           [ 1.2785e+00,  1.2785e+00,  1.2785e+00,  ...,  7.9499e-01,\n",
       "             7.9499e-01,  7.9499e-01],\n",
       "           ...,\n",
       "           [-1.2527e+00, -1.2527e+00, -1.2527e+00,  ..., -1.3807e+00,\n",
       "            -1.3807e+00, -1.3807e+00],\n",
       "           [-1.2527e+00, -1.2527e+00, -1.2527e+00,  ..., -1.3807e+00,\n",
       "            -1.3807e+00, -1.3807e+00],\n",
       "           [-1.2527e+00, -1.2527e+00, -1.2527e+00,  ..., -1.3807e+00,\n",
       "            -1.3807e+00, -1.3807e+00]]],\n",
       "\n",
       "\n",
       "         [[[ 1.7853e-01,  1.9312e-01,  2.0772e-01,  ...,  7.0407e-01,\n",
       "             7.0407e-01,  7.0407e-01],\n",
       "           [ 1.7853e-01,  1.9312e-01,  2.0772e-01,  ...,  7.0407e-01,\n",
       "             7.0407e-01,  7.0407e-01],\n",
       "           [ 1.7853e-01,  1.9312e-01,  2.0772e-01,  ...,  6.7487e-01,\n",
       "             6.7487e-01,  6.7487e-01],\n",
       "           ...,\n",
       "           [-1.6755e+00, -1.6901e+00, -1.7047e+00,  ..., -1.6463e+00,\n",
       "            -1.6609e+00, -1.6901e+00],\n",
       "           [-1.6901e+00, -1.7047e+00, -1.7047e+00,  ..., -1.6755e+00,\n",
       "            -1.6901e+00, -1.7047e+00],\n",
       "           [-1.6901e+00, -1.7047e+00, -1.7193e+00,  ..., -1.6755e+00,\n",
       "            -1.6901e+00, -1.7047e+00]],\n",
       "\n",
       "          [[ 4.3904e-01,  4.5404e-01,  4.3904e-01,  ...,  9.4930e-01,\n",
       "             9.4930e-01,  9.4930e-01],\n",
       "           [ 4.3904e-01,  4.5404e-01,  4.3904e-01,  ...,  9.4930e-01,\n",
       "             9.4930e-01,  9.4930e-01],\n",
       "           [ 4.3904e-01,  4.5404e-01,  4.3904e-01,  ...,  9.1929e-01,\n",
       "             9.1929e-01,  9.1929e-01],\n",
       "           ...,\n",
       "           [-1.5120e+00, -1.5270e+00, -1.5420e+00,  ..., -1.5270e+00,\n",
       "            -1.5420e+00, -1.5720e+00],\n",
       "           [-1.5270e+00, -1.5420e+00, -1.5420e+00,  ..., -1.5570e+00,\n",
       "            -1.5720e+00, -1.5870e+00],\n",
       "           [-1.5270e+00, -1.5420e+00, -1.5570e+00,  ..., -1.5570e+00,\n",
       "            -1.5720e+00, -1.5870e+00]],\n",
       "\n",
       "          [[ 6.1013e-01,  6.1013e-01,  5.9591e-01,  ...,  1.1078e+00,\n",
       "             1.1078e+00,  1.1078e+00],\n",
       "           [ 6.1013e-01,  6.1013e-01,  5.9591e-01,  ...,  1.1078e+00,\n",
       "             1.1078e+00,  1.1078e+00],\n",
       "           [ 6.1013e-01,  6.1013e-01,  5.9591e-01,  ...,  1.0794e+00,\n",
       "             1.0794e+00,  1.0794e+00],\n",
       "           ...,\n",
       "           [-1.1816e+00, -1.1958e+00, -1.2100e+00,  ..., -1.1816e+00,\n",
       "            -1.1958e+00, -1.2243e+00],\n",
       "           [-1.1958e+00, -1.2100e+00, -1.2100e+00,  ..., -1.2100e+00,\n",
       "            -1.2243e+00, -1.2385e+00],\n",
       "           [-1.1958e+00, -1.2100e+00, -1.2243e+00,  ..., -1.2100e+00,\n",
       "            -1.2243e+00, -1.2385e+00]]],\n",
       "\n",
       "\n",
       "         [[[ 3.2541e-02,  3.2541e-02,  3.2541e-02,  ...,  1.3473e-01,\n",
       "             1.3473e-01,  1.3473e-01],\n",
       "           [ 3.2541e-02,  3.2541e-02,  3.2541e-02,  ...,  1.3473e-01,\n",
       "             1.3473e-01,  1.3473e-01],\n",
       "           [ 3.2541e-02,  3.2541e-02,  3.2541e-02,  ...,  1.3473e-01,\n",
       "             1.3473e-01,  1.3473e-01],\n",
       "           ...,\n",
       "           [-4.2001e-01, -4.2001e-01, -4.2001e-01,  ..., -2.0103e-01,\n",
       "            -2.0103e-01, -2.0103e-01],\n",
       "           [-4.2001e-01, -4.2001e-01, -4.2001e-01,  ..., -2.3023e-01,\n",
       "            -2.3023e-01, -2.0103e-01],\n",
       "           [-4.2001e-01, -4.2001e-01, -4.2001e-01,  ..., -2.3023e-01,\n",
       "            -2.3023e-01, -2.0103e-01]],\n",
       "\n",
       "          [[ 2.4394e-01,  2.4394e-01,  2.4394e-01,  ...,  3.0397e-01,\n",
       "             3.0397e-01,  3.0397e-01],\n",
       "           [ 2.4394e-01,  2.4394e-01,  2.4394e-01,  ...,  3.0397e-01,\n",
       "             3.0397e-01,  3.0397e-01],\n",
       "           [ 2.4394e-01,  2.4394e-01,  2.4394e-01,  ...,  3.0397e-01,\n",
       "             3.0397e-01,  3.0397e-01],\n",
       "           ...,\n",
       "           [-2.9634e-01, -2.9634e-01, -2.9634e-01,  ...,  1.8820e-02,\n",
       "             1.8820e-02, -1.1196e-02],\n",
       "           [-2.9634e-01, -2.9634e-01, -2.9634e-01,  ..., -1.1196e-02,\n",
       "            -1.1196e-02, -2.6204e-02],\n",
       "           [-2.9634e-01, -2.9634e-01, -2.9634e-01,  ..., -1.1196e-02,\n",
       "            -1.1196e-02, -2.6204e-02]],\n",
       "\n",
       "          [[ 4.8215e-01,  4.8215e-01,  4.8215e-01,  ...,  5.5325e-01,\n",
       "             5.5325e-01,  5.5325e-01],\n",
       "           [ 4.8215e-01,  4.8215e-01,  4.8215e-01,  ...,  5.5325e-01,\n",
       "             5.5325e-01,  5.5325e-01],\n",
       "           [ 4.8215e-01,  4.8215e-01,  4.8215e-01,  ...,  5.5325e-01,\n",
       "             5.5325e-01,  5.5325e-01],\n",
       "           ...,\n",
       "           [-1.3329e-03, -1.3329e-03, -1.3329e-03,  ...,  2.2619e-01,\n",
       "             2.2619e-01,  2.1197e-01],\n",
       "           [-1.3329e-03, -1.3329e-03, -1.3329e-03,  ...,  1.9775e-01,\n",
       "             1.9775e-01,  1.9775e-01],\n",
       "           [-1.3329e-03, -1.3329e-03, -1.3329e-03,  ...,  1.9775e-01,\n",
       "             1.9775e-01,  1.9775e-01]]],\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "         [[[ 1.3473e-01,  1.2013e-01,  1.2013e-01,  ...,  4.1210e-01,\n",
       "             4.1210e-01,  4.2670e-01],\n",
       "           [ 1.3473e-01,  1.2013e-01,  1.2013e-01,  ...,  4.1210e-01,\n",
       "             4.1210e-01,  4.2670e-01],\n",
       "           [ 1.3473e-01,  1.2013e-01,  1.2013e-01,  ...,  4.1210e-01,\n",
       "             4.1210e-01,  4.2670e-01],\n",
       "           ...,\n",
       "           [ 1.6393e-01,  1.6393e-01,  1.4933e-01,  ...,  2.2232e-01,\n",
       "             2.0772e-01,  2.0772e-01],\n",
       "           [ 1.7853e-01,  1.6393e-01,  1.4933e-01,  ...,  2.2232e-01,\n",
       "             2.0772e-01,  2.0772e-01],\n",
       "           [ 1.7853e-01,  1.6393e-01,  1.4933e-01,  ...,  2.2232e-01,\n",
       "             2.0772e-01,  2.0772e-01]],\n",
       "\n",
       "          [[ 3.7901e-01,  3.6400e-01,  3.6400e-01,  ...,  6.3414e-01,\n",
       "             6.3414e-01,  6.4915e-01],\n",
       "           [ 3.7901e-01,  3.6400e-01,  3.6400e-01,  ...,  6.3414e-01,\n",
       "             6.3414e-01,  6.4915e-01],\n",
       "           [ 3.7901e-01,  3.6400e-01,  3.6400e-01,  ...,  6.3414e-01,\n",
       "             6.3414e-01,  6.4915e-01],\n",
       "           ...,\n",
       "           [ 3.4899e-01,  3.6400e-01,  3.7901e-01,  ...,  3.0397e-01,\n",
       "             2.8896e-01,  2.8896e-01],\n",
       "           [ 3.4899e-01,  3.6400e-01,  3.7901e-01,  ...,  3.0397e-01,\n",
       "             2.8896e-01,  2.8896e-01],\n",
       "           [ 3.4899e-01,  3.6400e-01,  3.7901e-01,  ...,  3.0397e-01,\n",
       "             2.8896e-01,  2.8896e-01]],\n",
       "\n",
       "          [[ 6.2435e-01,  6.1013e-01,  6.1013e-01,  ...,  8.8031e-01,\n",
       "             8.8031e-01,  8.6609e-01],\n",
       "           [ 6.2435e-01,  6.1013e-01,  6.1013e-01,  ...,  8.8031e-01,\n",
       "             8.8031e-01,  8.6609e-01],\n",
       "           [ 6.2435e-01,  6.1013e-01,  6.1013e-01,  ...,  8.8031e-01,\n",
       "             8.8031e-01,  8.6609e-01],\n",
       "           ...,\n",
       "           [ 5.5325e-01,  5.5325e-01,  5.6747e-01,  ...,  5.3903e-01,\n",
       "             5.2481e-01,  5.2481e-01],\n",
       "           [ 5.5325e-01,  5.6747e-01,  5.6747e-01,  ...,  5.3903e-01,\n",
       "             5.2481e-01,  5.2481e-01],\n",
       "           [ 5.5325e-01,  5.6747e-01,  5.6747e-01,  ...,  5.3903e-01,\n",
       "             5.2481e-01,  5.2481e-01]]],\n",
       "\n",
       "\n",
       "         [[[ 7.6336e-02,  7.6336e-02,  7.6336e-02,  ...,  3.9750e-01,\n",
       "             3.8290e-01,  3.8290e-01],\n",
       "           [ 9.0935e-02,  9.0935e-02,  9.0935e-02,  ...,  3.8290e-01,\n",
       "             3.8290e-01,  3.8290e-01],\n",
       "           [ 9.0935e-02,  9.0935e-02,  9.0935e-02,  ...,  4.1210e-01,\n",
       "             3.9750e-01,  3.8290e-01],\n",
       "           ...,\n",
       "           [ 2.3692e-01,  2.3692e-01,  2.5152e-01,  ...,  6.1738e-02,\n",
       "             6.1738e-02,  7.6336e-02],\n",
       "           [ 2.3692e-01,  2.2232e-01,  2.3692e-01,  ...,  6.1738e-02,\n",
       "             6.1738e-02,  7.6336e-02],\n",
       "           [ 2.3692e-01,  2.2232e-01,  2.3692e-01,  ...,  6.1738e-02,\n",
       "             6.1738e-02,  7.6336e-02]],\n",
       "\n",
       "          [[ 3.1897e-01,  3.1897e-01,  3.1897e-01,  ...,  6.1913e-01,\n",
       "             6.0412e-01,  5.7411e-01],\n",
       "           [ 3.3398e-01,  3.3398e-01,  3.3398e-01,  ...,  6.0412e-01,\n",
       "             6.0412e-01,  5.7411e-01],\n",
       "           [ 3.3398e-01,  3.3398e-01,  3.3398e-01,  ...,  5.8911e-01,\n",
       "             5.5910e-01,  5.5910e-01],\n",
       "           ...,\n",
       "           [ 4.2403e-01,  4.2403e-01,  4.0902e-01,  ...,  2.4394e-01,\n",
       "             2.4394e-01,  2.2893e-01],\n",
       "           [ 4.2403e-01,  4.0902e-01,  3.9401e-01,  ...,  2.4394e-01,\n",
       "             2.4394e-01,  2.2893e-01],\n",
       "           [ 4.2403e-01,  4.0902e-01,  3.9401e-01,  ...,  2.4394e-01,\n",
       "             2.4394e-01,  2.2893e-01]],\n",
       "\n",
       "          [[ 5.3903e-01,  5.5325e-01,  5.6747e-01,  ...,  8.3765e-01,\n",
       "             8.2343e-01,  8.0921e-01],\n",
       "           [ 5.5325e-01,  5.6747e-01,  5.8169e-01,  ...,  8.2343e-01,\n",
       "             8.2343e-01,  8.0921e-01],\n",
       "           [ 5.5325e-01,  5.6747e-01,  5.8169e-01,  ...,  8.2343e-01,\n",
       "             8.0921e-01,  7.9499e-01],\n",
       "           ...,\n",
       "           [ 6.2435e-01,  6.2435e-01,  6.2435e-01,  ...,  4.5371e-01,\n",
       "             4.5371e-01,  4.5371e-01],\n",
       "           [ 6.2435e-01,  6.1013e-01,  6.1013e-01,  ...,  4.5371e-01,\n",
       "             4.5371e-01,  4.5371e-01],\n",
       "           [ 6.2435e-01,  6.1013e-01,  6.1013e-01,  ...,  4.5371e-01,\n",
       "             4.5371e-01,  4.5371e-01]]],\n",
       "\n",
       "\n",
       "         [[[ 5.4349e-01,  5.7268e-01,  5.8728e-01,  ...,  9.2304e-01,\n",
       "             9.2304e-01,  9.3764e-01],\n",
       "           [ 5.4349e-01,  5.7268e-01,  5.8728e-01,  ...,  9.9604e-01,\n",
       "             9.9604e-01,  1.0106e+00],\n",
       "           [ 5.4349e-01,  5.7268e-01,  5.8728e-01,  ...,  1.0106e+00,\n",
       "             1.0106e+00,  1.0252e+00],\n",
       "           ...,\n",
       "           [-1.2804e-01, -1.2804e-01, -1.2804e-01,  ..., -9.8845e-02,\n",
       "            -1.2804e-01, -1.2804e-01],\n",
       "           [-1.2804e-01, -1.2804e-01, -1.2804e-01,  ..., -1.2804e-01,\n",
       "            -1.2804e-01, -1.4264e-01],\n",
       "           [-1.2804e-01, -1.2804e-01, -1.2804e-01,  ..., -1.2804e-01,\n",
       "            -1.4264e-01, -1.4264e-01]],\n",
       "\n",
       "          [[ 7.8422e-01,  8.1423e-01,  8.2924e-01,  ...,  1.0694e+00,\n",
       "             1.0694e+00,  1.0844e+00],\n",
       "           [ 7.8422e-01,  8.1423e-01,  8.2924e-01,  ...,  1.1444e+00,\n",
       "             1.1444e+00,  1.1594e+00],\n",
       "           [ 7.8422e-01,  8.1423e-01,  8.2924e-01,  ...,  1.1594e+00,\n",
       "             1.1594e+00,  1.1744e+00],\n",
       "           ...,\n",
       "           [ 9.3858e-02,  9.3858e-02,  9.3858e-02,  ...,  7.8851e-02,\n",
       "             4.8835e-02,  4.8835e-02],\n",
       "           [ 9.3858e-02,  9.3858e-02,  9.3858e-02,  ...,  4.8835e-02,\n",
       "             4.8835e-02,  3.3827e-02],\n",
       "           [ 9.3858e-02,  9.3858e-02,  9.3858e-02,  ...,  4.8835e-02,\n",
       "             3.3827e-02,  3.3827e-02]],\n",
       "\n",
       "          [[ 9.5141e-01,  9.7985e-01,  9.9407e-01,  ...,  1.1932e+00,\n",
       "             1.1932e+00,  1.2074e+00],\n",
       "           [ 9.5141e-01,  9.7985e-01,  9.9407e-01,  ...,  1.2643e+00,\n",
       "             1.2643e+00,  1.2785e+00],\n",
       "           [ 9.5141e-01,  9.7985e-01,  9.9407e-01,  ...,  1.2785e+00,\n",
       "             1.2785e+00,  1.2927e+00],\n",
       "           ...,\n",
       "           [ 2.9729e-01,  2.9729e-01,  2.9729e-01,  ...,  2.9729e-01,\n",
       "             2.6885e-01,  2.6885e-01],\n",
       "           [ 2.9729e-01,  2.9729e-01,  2.9729e-01,  ...,  2.6885e-01,\n",
       "             2.6885e-01,  2.5463e-01],\n",
       "           [ 2.9729e-01,  2.9729e-01,  2.9729e-01,  ...,  2.6885e-01,\n",
       "             2.5463e-01,  2.5463e-01]]]]], device='cuda:0'), 'seg_tokens': tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]]], device='cuda:0',\n",
       "       dtype=torch.float16)}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['pixel_values_videos'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of image tokens in input_ids (0) different from num_images (1).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m generate_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_new_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m100\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_sample\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.9\u001b[39m}\n\u001b[0;32m----> 3\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mbatch_decode(output, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/envs/llasa/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/llasa/lib/python3.11/site-packages/transformers/generation/utils.py:2020\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2012\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2013\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2014\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2015\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2016\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2017\u001b[0m     )\n\u001b[1;32m   2019\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2020\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2021\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2022\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2024\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2025\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2027\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2028\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2030\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2031\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2032\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2033\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2034\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2039\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2040\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/llasa/lib/python3.11/site-packages/transformers/generation/utils.py:2966\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2963\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   2965\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2966\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2968\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2969\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/llasa/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/llasa/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/llasa/lib/python3.11/site-packages/accelerate/hooks.py:169\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/workspace/LLaSA/model/llasa_arch.py:216\u001b[0m, in \u001b[0;36mLLaSA.forward\u001b[0;34m(self, input_ids, pixel_values, pixel_values_videos, seg_tokens, image_sizes, attention_mask, position_ids, past_key_values, inputs_embeds, vision_feature_layer, vision_feature_select_strategy, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m features, lens, special_token \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m             (\n\u001b[1;32m    211\u001b[0m                 inputs_embeds,\n\u001b[1;32m    212\u001b[0m                 attention_mask,\n\u001b[1;32m    213\u001b[0m                 position_ids,\n\u001b[1;32m    214\u001b[0m                 labels,\n\u001b[1;32m    215\u001b[0m                 input_ids,\n\u001b[0;32m--> 216\u001b[0m             ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_input_ids_with_image_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m                \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m                \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m                \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m                \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m                \u001b[49m\u001b[43mimage_token_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspecial_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# Retrieve the first layer to inspect the logits and mask out the hidden states that are set to 0\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     first_layer_past_key_value \u001b[38;5;241m=\u001b[39m past_key_values[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m][:, :, :, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/envs/llasa/lib/python3.11/site-packages/transformers/models/llava_next_video/modeling_llava_next_video.py:602\u001b[0m, in \u001b[0;36mLlavaNextVideoForConditionalGeneration._merge_input_ids_with_image_features\u001b[0;34m(self, image_features, feature_lens, inputs_embeds, input_ids, attention_mask, position_ids, labels, image_token_index, ignore_index)\u001b[0m\n\u001b[1;32m    600\u001b[0m total_num_special_image_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(special_image_token_mask)\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_num_special_image_tokens \u001b[38;5;241m!=\u001b[39m num_images:\n\u001b[0;32m--> 602\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    603\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of image tokens in input_ids (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_num_special_image_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) different from num_images (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_images\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    604\u001b[0m     )\n\u001b[1;32m    605\u001b[0m \u001b[38;5;66;03m# Compute the maximum embed dimension\u001b[39;00m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;66;03m# max_image_feature_lens is max_feature_lens per batch\u001b[39;00m\n\u001b[1;32m    607\u001b[0m feature_lens \u001b[38;5;241m=\u001b[39m feature_lens\u001b[38;5;241m.\u001b[39mto(input_ids\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mValueError\u001b[0m: Number of image tokens in input_ids (0) different from num_images (1)."
     ]
    }
   ],
   "source": [
    "generate_kwargs = {\"max_new_tokens\": 100, \"do_sample\": True, \"top_p\": 0.9}\n",
    "\n",
    "output = model.generate(**inputs, **generate_kwargs)\n",
    "generated_text = processor.batch_decode(output, skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pynvml\n",
      "  Downloading pynvml-11.5.3-py3-none-any.whl.metadata (8.8 kB)\n",
      "Downloading pynvml-11.5.3-py3-none-any.whl (53 kB)\n",
      "Installing collected packages: pynvml\n",
      "Successfully installed pynvml-11.5.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pynvml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynvml import *\n",
    "\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m logging\u001b[38;5;241m.\u001b[39mset_verbosity_error()\n\u001b[1;32m     13\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdefault_args)\n\u001b[0;32m---> 14\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model\u001b[38;5;241m=\u001b[39mmodel, args\u001b[38;5;241m=\u001b[39mtraining_args, train_dataset\u001b[38;5;241m=\u001b[39m\u001b[43mds\u001b[49m)\n\u001b[1;32m     15\u001b[0m result \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     16\u001b[0m print_summary(result)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ds' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, logging\n",
    "default_args = {\n",
    "    \"output_dir\": \"tmp\",\n",
    "    \"eval_strategy\": \"steps\",\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"log_level\": \"error\",\n",
    "    \"report_to\": \"none\",\n",
    "}\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(per_device_train_batch_size=4, **default_args)\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=ds)\n",
    "result = trainer.train()\n",
    "print_summary(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s> USER: <video>\\nWhy is this video funny? <seg>\\nASSISTANT:\\nod? groups in the equipments available. The keys availables?s are nots in the meanings, The birds?</s>']\n"
     ]
    }
   ],
   "source": [
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config['seg_tok\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# Load the .mat file\n",
    "data = loadmat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Open the .mat file\n",
    "with h5py.File('/workspace/LLaSA/dataset/A2D/a2d_annotation_with_instances/ouFmCnCh1wk/00030.h5', 'r') as f:\n",
    "    # List all variables stored in the .mat file\n",
    "    print(\"Keys in the file:\", list(f.keys()))\n",
    "\n",
    "\n",
    "    def print_structure(name, obj):\n",
    "        \"\"\"Print the structure of the HDF5 file\"\"\"\n",
    "        if isinstance(obj, h5py.Dataset):\n",
    "            print(f\"Dataset: {name}, Shape: {obj.shape}, Data type: {obj.dtype}\")\n",
    "        elif isinstance(obj, h5py.Group):\n",
    "            print(f\"Group: {name}\")\n",
    "\n",
    "    # Visit each object in the file\n",
    "    f.visititems(print_structure)\n",
    "\n",
    "    coord = f['reBBox'][:]\n",
    "    print(f['instance'][:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input_ids.shape[1] != 1:\n",
    "    iterator = (\n",
    "        (image_features, feature_lens, self.config.image_token_index),\n",
    "        (video_features, video_feature_lens, self.config.video_token_index),\n",
    "    )\n",
    "    for features, lens, special_token in iterator:\n",
    "        if features is not None:\n",
    "            print('before:',\n",
    "                inputs_embeds.shape,\n",
    "                attention_mask.shape,\n",
    "                position_ids,\n",
    "                labels,\n",
    "                input_ids)\n",
    "            (\n",
    "                inputs_embeds,\n",
    "                attention_mask,\n",
    "                position_ids,\n",
    "                labels,\n",
    "                input_ids,\n",
    "            ) = self._merge_input_ids_with_image_features(\n",
    "                features,\n",
    "                lens,\n",
    "                inputs_embeds,\n",
    "                input_ids,\n",
    "                attention_mask,\n",
    "                position_ids,\n",
    "                labels=labels,\n",
    "                image_token_index=special_token,\n",
    "            )\n",
    "            print('after:',\n",
    "                inputs_embeds.shape,\n",
    "                attention_mask.shape,\n",
    "                position_ids,\n",
    "                labels,\n",
    "                input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --no-cache-dir opencv-contrib-python \n",
    "!pip install  --no-cache-dir opencv-python\n",
    "!pip install --no-cache-dir opencv-python-headless\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def draw_bounding_box(image_path, bbox):\n",
    "    \"\"\"\n",
    "    Retrieve an image from a local path and draw a bounding box on it using matplotlib.\n",
    "\n",
    "    :param image_path: Path to the local image file\n",
    "    :param bbox: Bounding box coordinates in the format (x_min, y_min, x_max, y_max)\n",
    "    \"\"\"\n",
    "    # Load the image using PIL\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Create a figure and axis\n",
    "    fig, ax = plt.subplots(1)\n",
    "\n",
    "    # Display the image\n",
    "    ax.imshow(image)\n",
    "\n",
    "    # Extract bounding box coordinates\n",
    "    x_min, y_min, x_max, y_max = bbox\n",
    "\n",
    "    # Create a Rectangle patch\n",
    "    rect = patches.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min,\n",
    "                             linewidth=2, edgecolor='g', facecolor='none')\n",
    "\n",
    "    # Add the rectangle to the plot\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "image_path = '/workspace/LLaSA/dataset/A2D/clips320jpeg/ouFmCnCh1wk/00030.jpeg'  # Replace with the path to your image file\n",
    "num = 2\n",
    "bbox = tuple(sub_coord[num] for sub_coord in coord)  # Example bounding box coordinates (x_min, y_min, x_max, y_max)\n",
    "\n",
    "draw_bounding_box(image_path, bbox)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class VideoCaptionDataset(Dataset):\n",
    "    def __init__(self, data, pt_file_dir, start_idx, end_idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (dict): A dictionary containing video metadata.\n",
    "            pt_file_dir (str): Directory path where .pt files are stored.\n",
    "            start_idx (int): Starting index of the keys to include in the dataset.\n",
    "            end_idx (int): Ending index (exclusive) of the keys to include in the dataset.\n",
    "        \"\"\"\n",
    "        self.pt_file_dir = pt_file_dir\n",
    "        self.keys = list(data.keys())[start_idx:end_idx]  # Get the subset of keys\n",
    "        self.data = {key: data[key] for key in self.keys}  # Subset of data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        key = self.keys[idx]\n",
    "        item = self.data[key]\n",
    "        \n",
    "        # Extract data\n",
    "        caption = item['caption']\n",
    "        bbox = torch.tensor(item['bbox'])  # Convert bbox to tensor\n",
    "        video_id = item['video']\n",
    "        pt_file_path = f\"{self.pt_file_dir}/{video_id}.pt\"  # Assuming .pt files are named after video IDs\n",
    "\n",
    "        # Load the pt file (this will load a tensor)\n",
    "        features = torch.load(pt_file_path)\n",
    "\n",
    "        return caption, bbox, features, video_id\n",
    "\n",
    "# Load JSON data\n",
    "json_file_path = 'path/to/your/data.json'  # Path to your JSON file\n",
    "pt_file_dir = 'path/to/pt/files'           # Directory containing .pt files\n",
    "\n",
    "with open(json_file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create training and testing datasets\n",
    "train_dataset = VideoCaptionDataset(data, pt_file_dir, start_idx=0, end_idx=2000)\n",
    "test_dataset = VideoCaptionDataset(data, pt_file_dir, start_idx=2000, end_idx=2500)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Iterate over the train DataLoader\n",
    "for batch in train_loader:\n",
    "    captions, bboxes, features, video_ids = batch\n",
    "    print(\"Training batch - Captions:\", captions)\n",
    "    print(\"Training batch - Bounding boxes:\", bboxes)\n",
    "    print(\"Training batch - Features shape:\", features.shape)\n",
    "    print(\"Training batch - Video IDs:\", video_ids)\n",
    "    break  # Just show the first batch for demonstration\n",
    "\n",
    "# Iterate over the test DataLoader\n",
    "for batch in test_loader:\n",
    "    captions, bboxes, features, video_ids = batch\n",
    "    print(\"Testing batch - Captions:\", captions)\n",
    "    print(\"Testing batch - Bounding boxes:\", bboxes)\n",
    "    print(\"Testing batch - Features shape:\", features.shape)\n",
    "    print(\"Testing batch - Video IDs:\", video_ids)\n",
    "    break  # Just show the first batch for demonstratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llasa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
