{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/LLaSA\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('../..')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LlavaNextVideoProcessor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m llava_processor \u001b[38;5;241m=\u001b[39m \u001b[43mLlavaNextVideoProcessor\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/workspace/LLaSA/checkpoints/LLaVA-NeXT-Video-7B-hf\u001b[39m\u001b[38;5;124m\"\u001b[39m, cache_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/workspace/LLaSA/checkpoints\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LlavaNextVideoProcessor' is not defined"
     ]
    }
   ],
   "source": [
    "llava_processor = LlavaNextVideoProcessor.from_pretrained(\"/workspace/LLaSA/checkpoints/LLaVA-NeXT-Video-7B-hf\", cache_dir='/workspace/LLaSA/checkpoints')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.llava_next_video.processing_llava_next_video.LlavaNextVideoProcessor"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(llava_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlavaNextVideoProcessor:\n",
       "- video_processor: LlavaNextVideoImageProcessor {\n",
       "  \"crop_size\": {\n",
       "    \"height\": 336,\n",
       "    \"width\": 336\n",
       "  },\n",
       "  \"do_center_crop\": true,\n",
       "  \"do_convert_rgb\": true,\n",
       "  \"do_normalize\": true,\n",
       "  \"do_pad\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_grid_pinpoints\": [\n",
       "    [\n",
       "      336,\n",
       "      672\n",
       "    ],\n",
       "    [\n",
       "      672,\n",
       "      336\n",
       "    ],\n",
       "    [\n",
       "      672,\n",
       "      672\n",
       "    ],\n",
       "    [\n",
       "      1008,\n",
       "      336\n",
       "    ],\n",
       "    [\n",
       "      336,\n",
       "      1008\n",
       "    ]\n",
       "  ],\n",
       "  \"image_mean\": [\n",
       "    0.48145466,\n",
       "    0.4578275,\n",
       "    0.40821073\n",
       "  ],\n",
       "  \"image_processor_type\": \"LlavaNextVideoImageProcessor\",\n",
       "  \"image_std\": [\n",
       "    0.26862954,\n",
       "    0.26130258,\n",
       "    0.27577711\n",
       "  ],\n",
       "  \"processor_class\": \"LlavaNextVideoProcessor\",\n",
       "  \"resample\": 3,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"shortest_edge\": 336\n",
       "  }\n",
       "}\n",
       "\n",
       "- image_processor: LlavaNextImageProcessor {\n",
       "  \"crop_size\": {\n",
       "    \"height\": 336,\n",
       "    \"width\": 336\n",
       "  },\n",
       "  \"do_center_crop\": true,\n",
       "  \"do_convert_rgb\": true,\n",
       "  \"do_normalize\": true,\n",
       "  \"do_pad\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_grid_pinpoints\": [\n",
       "    [\n",
       "      336,\n",
       "      672\n",
       "    ],\n",
       "    [\n",
       "      672,\n",
       "      336\n",
       "    ],\n",
       "    [\n",
       "      672,\n",
       "      672\n",
       "    ],\n",
       "    [\n",
       "      1008,\n",
       "      336\n",
       "    ],\n",
       "    [\n",
       "      336,\n",
       "      1008\n",
       "    ]\n",
       "  ],\n",
       "  \"image_mean\": [\n",
       "    0.48145466,\n",
       "    0.4578275,\n",
       "    0.40821073\n",
       "  ],\n",
       "  \"image_processor_type\": \"LlavaNextImageProcessor\",\n",
       "  \"image_std\": [\n",
       "    0.26862954,\n",
       "    0.26130258,\n",
       "    0.27577711\n",
       "  ],\n",
       "  \"processor_class\": \"LlavaNextVideoProcessor\",\n",
       "  \"resample\": 3,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"shortest_edge\": 336\n",
       "  }\n",
       "}\n",
       "\n",
       "- tokenizer: LlamaTokenizerFast(name_or_path='llava-hf/LLaVA-NeXT-Video-7B-hf', vocab_size=32000, model_max_length=4096, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32000: AddedToken(\"<video>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32001: AddedToken(\"<image>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       "\n",
       "{\n",
       "  \"chat_template\": \"{% for message in messages %}{% if message['role'] != 'system' %}{{ message['role'].upper() + ': '}}{% endif %}{# Render all images first #}{% for content in message['content'] | selectattr('type', 'equalto', 'image') %}{{ '<image>\\n' }}{% endfor %}{# Render all images first #}{% for content in message['content'] | selectattr('type', 'equalto', 'video') %}{{ '<video>\\n' }}{% endfor %}{# Render all text next #}{% if message['role'] != 'assistant' %}{% for content in message['content'] | selectattr('type', 'equalto', 'text') %}{{ content['text'] + ' '}}{% endfor %}{% else %}{% for content in message['content'] | selectattr('type', 'equalto', 'text') %}{% generation %}{{ content['text'] + ' '}}{% endgeneration %}{% endfor %}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ 'ASSISTANT:' }}{% endif %}\",\n",
       "  \"image_token\": \"<image>\",\n",
       "  \"patch_size\": null,\n",
       "  \"processor_class\": \"LlavaNextVideoProcessor\",\n",
       "  \"video_token\": \"<video>\",\n",
       "  \"vision_feature_select_strategy\": null\n",
       "}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Received a NoneType for argument tokenizer, but a ('LlamaTokenizer', 'LlamaTokenizerFast') was expected.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 17\u001b[0m\n\u001b[1;32m     10\u001b[0m importlib\u001b[38;5;241m.\u001b[39mreload(importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.llasa_processor\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     12\u001b[0m quantization_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m     13\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m     bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16\n\u001b[1;32m     15\u001b[0m )\n\u001b[0;32m---> 17\u001b[0m processor \u001b[38;5;241m=\u001b[39m \u001b[43mLlasaProcessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/workspace/LLaSA/checkpoints/LLaVA-NeXT-Video-7B-hf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/workspace/LLaSA/checkpoints\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m model \u001b[38;5;241m=\u001b[39m LLaSA\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/workspace/LLaSA/checkpoints/LLaVA-NeXT-Video-7B-hf\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     20\u001b[0m     quantization_config\u001b[38;5;241m=\u001b[39mquantization_config,\n\u001b[1;32m     21\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     22\u001b[0m     cache_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/workspace/LLaSA/checkpoints\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     23\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/llasa/lib/python3.11/site-packages/transformers/processing_utils.py:895\u001b[0m, in \u001b[0;36mProcessorMixin.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[1;32m    892\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_arguments_from_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    893\u001b[0m processor_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mget_processor_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 895\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_args_and_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/llasa/lib/python3.11/site-packages/transformers/processing_utils.py:712\u001b[0m, in \u001b[0;36mProcessorMixin.from_args_and_dict\u001b[0;34m(cls, args, processor_dict, **kwargs)\u001b[0m\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m processor_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    711\u001b[0m unused_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_init_kwargs(processor_config\u001b[38;5;241m=\u001b[39mprocessor_dict, valid_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_kwargs)\n\u001b[0;32m--> 712\u001b[0m processor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprocessor_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chat_template \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    714\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(processor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_template\u001b[39m\u001b[38;5;124m\"\u001b[39m, chat_template)\n",
      "File \u001b[0;32m/workspace/LLaSA/model/llasa_processor.py:51\u001b[0m, in \u001b[0;36mLlasaProcessor.__init__\u001b[0;34m(self, video_processor, image_processor, seg_processor, tokenizer, chat_template, patch_size, vision_feature_select_strategy, video_token, image_token, seg_token, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     37\u001b[0m     video_processor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     48\u001b[0m ):\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mprint\u001b[39m(tokenizer)\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvideo_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideo_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchat_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchat_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvision_feature_select_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvision_feature_select_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvideo_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideo_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseg_processor \u001b[38;5;241m=\u001b[39m seg_processor\n",
      "File \u001b[0;32m/opt/conda/envs/llasa/lib/python3.11/site-packages/transformers/models/llava_next_video/processing_llava_next_video.py:86\u001b[0m, in \u001b[0;36mLlavaNextVideoProcessor.__init__\u001b[0;34m(self, video_processor, image_processor, tokenizer, chat_template, patch_size, vision_feature_select_strategy, video_token, image_token, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_token \u001b[38;5;241m=\u001b[39m image_token\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo_token \u001b[38;5;241m=\u001b[39m video_token\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvideo_processor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_processor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchat_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchat_template\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/llasa/lib/python3.11/site-packages/transformers/processing_utils.py:359\u001b[0m, in \u001b[0;36mProcessorMixin.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m     proper_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(transformers_module, class_name)\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, proper_class):\n\u001b[0;32m--> 359\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    360\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(arg)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for argument \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattribute_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m was expected.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    361\u001b[0m     )\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attribute_name, arg)\n",
      "\u001b[0;31mTypeError\u001b[0m: Received a NoneType for argument tokenizer, but a ('LlamaTokenizer', 'LlamaTokenizerFast') was expected."
     ]
    }
   ],
   "source": [
    "# reload module \n",
    "import importlib\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig, LlavaNextVideoForConditionalGeneration\n",
    "from model.llasa_arch import LLaSA\n",
    "from model.llasa_processor import LlasaProcessor, LlavaNextVideoProcessor\n",
    "\n",
    "\n",
    "importlib.reload(importlib.import_module(\"model.llasa_arch\"))\n",
    "importlib.reload(importlib.import_module(\"model.llasa_processor\"))\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "processor = LlavaNextVideoProcessor.from_pretrained(\"/workspace/LLaSA/checkpoints/LLaVA-NeXT-Video-7B-hf\", cache_dir='/workspace/LLaSA/checkpoints')\n",
    "model = LLaSA.from_pretrained(\n",
    "    \"/workspace/LLaSA/checkpoints/LLaVA-NeXT-Video-7B-hf\",\n",
    "    quantization_config=quantization_config,\n",
    "    device_map='auto',\n",
    "    cache_dir = '/workspace/LLaSA/checkpoints'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade -q accelerate bitsandbytes\n",
    "! pip install protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='linear': {'type'}\n",
      "Loading checkpoint shards:   0%|          | 0/3 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 11\u001b[0m\n\u001b[1;32m      5\u001b[0m quantization_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m      6\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m     bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m processor \u001b[38;5;241m=\u001b[39m LlavaNextVideoProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllava-hf/LLaVA-NeXT-Video-7B-hf\u001b[39m\u001b[38;5;124m\"\u001b[39m, cache_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/workspace/LLaSA/checkpoints\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLLaSA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllava-hf/LLaVA-NeXT-Video-7B-hf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/workspace/LLaSA/checkpoints\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     16\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/llasa/lib/python3.11/site-packages/transformers/modeling_utils.py:3956\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3946\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3947\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3949\u001b[0m     (\n\u001b[1;32m   3950\u001b[0m         model,\n\u001b[1;32m   3951\u001b[0m         missing_keys,\n\u001b[1;32m   3952\u001b[0m         unexpected_keys,\n\u001b[1;32m   3953\u001b[0m         mismatched_keys,\n\u001b[1;32m   3954\u001b[0m         offload_index,\n\u001b[1;32m   3955\u001b[0m         error_msgs,\n\u001b[0;32m-> 3956\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3958\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3959\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3960\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3961\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3962\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3963\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3964\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3965\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3967\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3968\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgguf_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3973\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3975\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   3976\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m/opt/conda/envs/llasa/lib/python3.11/site-packages/transformers/modeling_utils.py:4430\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path)\u001b[0m\n\u001b[1;32m   4426\u001b[0m                 set_module_tensor_to_device(\n\u001b[1;32m   4427\u001b[0m                     model_to_load, key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m*\u001b[39mparam\u001b[38;5;241m.\u001b[39msize(), dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   4428\u001b[0m                 )\n\u001b[1;32m   4429\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4430\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4431\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4432\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4433\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloaded_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4434\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4435\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4436\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4437\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4438\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4439\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4440\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4441\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4442\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4443\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4444\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4445\u001b[0m \u001b[43m            \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4446\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4447\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[1;32m   4448\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4449\u001b[0m     \u001b[38;5;66;03m# Sharded checkpoint or whole but low_cpu_mem_usage==True\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/llasa/lib/python3.11/site-packages/transformers/modeling_utils.py:964\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys, pretrained_model_name_or_path)\u001b[0m\n\u001b[1;32m    962\u001b[0m     set_module_tensor_to_device(model, param_name, param_device, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mset_module_kwargs)\n\u001b[1;32m    963\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 964\u001b[0m     \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_quantized_param\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;66;03m# For quantized modules with FSDP/DeepSpeed Stage 3, we need to quantize the parameter on the GPU\u001b[39;00m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;66;03m# and then cast it to CPU to avoid excessive memory usage on each GPU\u001b[39;00m\n\u001b[1;32m    967\u001b[0m     \u001b[38;5;66;03m# in comparison to the sharded model across GPUs.\u001b[39;00m\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_fsdp_enabled() \u001b[38;5;129;01mor\u001b[39;00m is_deepspeed_zero3_enabled():\n",
      "File \u001b[0;32m/opt/conda/envs/llasa/lib/python3.11/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:225\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.create_quantized_param\u001b[0;34m(self, model, param_value, param_name, target_device, state_dict, unexpected_keys)\u001b[0m\n\u001b[1;32m    222\u001b[0m         new_value \u001b[38;5;241m=\u001b[39m new_value\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    224\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m old_value\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n\u001b[0;32m--> 225\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mParams4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m new_value\n",
      "File \u001b[0;32m/opt/conda/envs/llasa/lib/python3.11/site-packages/bitsandbytes/nn/modules.py:332\u001b[0m, in \u001b[0;36mParams4bit.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m device, dtype, non_blocking, convert_to_format \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39m_parse_to(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m device\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbnb_quantized:\n\u001b[0;32m--> 332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_quantize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/llasa/lib/python3.11/site-packages/bitsandbytes/nn/modules.py:296\u001b[0m, in \u001b[0;36mParams4bit._quantize\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_quantize\u001b[39m(\u001b[38;5;28mself\u001b[39m, device):\n\u001b[0;32m--> 296\u001b[0m     w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m     w_4bit, quant_state \u001b[38;5;241m=\u001b[39m bnb\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mquantize_4bit(\n\u001b[1;32m    298\u001b[0m         w,\n\u001b[1;32m    299\u001b[0m         blocksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocksize,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    302\u001b[0m         quant_storage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_storage,\n\u001b[1;32m    303\u001b[0m     )\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m w_4bit\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig, LlavaNextVideoForConditionalGeneration, LlavaNextVideoProcessor\n",
    "import torch\n",
    "from model.llasa_arch import LLaSA\n",
    "from model.llasa_processor import LlasaProcessor\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "processor = LlasaProcessor.from_pretrained(\"llava-hf/LLaVA-NeXT-Video-7B-hf\", cache_dir='/workspace/LLaSA/checkpoints')\n",
    "model = LLaSA.from_pretrained(\n",
    "    \"llava-hf/LLaVA-NeXT-Video-7B-hf\",\n",
    "    quantization_config=quantization_config,\n",
    "    device_map='auto',\n",
    "    cache_dir = '/workspace/LLaSA/checkpoints'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import av\n",
    "import numpy as np\n",
    "\n",
    "def read_video_pyav(container, indices):\n",
    "    '''\n",
    "    Decode the video with PyAV decoder.\n",
    "\n",
    "    Args:\n",
    "        container (av.container.input.InputContainer): PyAV container.\n",
    "        indices (List[int]): List of frame indices to decode.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: np array of decoded frames of shape (num_frames, height, width, 3).\n",
    "    '''\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            frames.append(frame)\n",
    "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Download video from the hub\n",
    "#video_path_1 = hf_hub_download(repo_id=\"raushan-testing-hf/videos-test\", filename=\"sample_demo_1.mp4\", repo_type=\"dataset\")\n",
    "video_path_2 = hf_hub_download(repo_id=\"raushan-testing-hf/videos-test\", filename=\"karate.mp4\", repo_type=\"dataset\")\n",
    "\n",
    "video_path_1 = '/workspace/LLaSA/dataset/A2D/clips320H/__KkKB4wzrY.mp4'\n",
    "\n",
    "container = av.open(video_path_1)\n",
    "\n",
    "# sample uniformly 8 frames from the video (we can sample more for longer videos)\n",
    "total_frames = container.streams.video[0].frames\n",
    "indices = np.arange(0, total_frames, total_frames / 8).astype(int)\n",
    "clip_baby = read_video_pyav(container, indices)\n",
    "\n",
    "\n",
    "container = av.open(video_path_2)\n",
    "\n",
    "# sample uniformly 8 frames from the video (we can sample more for longer videos)\n",
    "total_frames = container.streams.video[0].frames\n",
    "indices = np.arange(0, total_frames, total_frames / 8).astype(int)\n",
    "clip_karate = read_video_pyav(container, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "# np array with shape (frames, height, width, channels)\n",
    "video = clip_baby\n",
    "\n",
    "fig = plt.figure()\n",
    "im = plt.imshow(video[0,:,:,:])\n",
    "\n",
    "plt.close() # this is required to not display the generated image\n",
    "\n",
    "def init():\n",
    "    im.set_data(video[0,:,:,:])\n",
    "\n",
    "def animate(i):\n",
    "    im.set_data(video[i,:,:,:])\n",
    "    return im\n",
    "\n",
    "#anim = animation.FuncAnimation(fig, animate, init_func=init, frames=video.shape[0],\n",
    "#                               interval=100)\n",
    "#HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets also load 2 images for generation from image data\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "image_stop = Image.open(requests.get(\"https://www.ilankelman.org/stopsigns/australia.jpg\", stream=True).raw)\n",
    "image_snowman = Image.open(requests.get(\"https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/snowman.jpg\", stream=True).raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each \"content\" is a list of dicts and you can add image/video/text modalities\n",
    "conversation = [\n",
    "      {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": [\n",
    "              {\"type\": \"text\", \"text\": \"Why is this video funny?\"},\n",
    "              {\"type\": \"video\"},\n",
    "              ],\n",
    "      },\n",
    "]\n",
    "\n",
    "conversation_2 = [\n",
    "      {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": [\n",
    "              {\"type\": \"text\", \"text\": \"What do you see in this video?\"},\n",
    "              {\"type\": \"video\"},\n",
    "              ],\n",
    "      },\n",
    "]\n",
    "\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "prompt_2 = processor.apply_chat_template(conversation_2, add_generation_prompt=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expanding inputs for image/video tokens in LLaVa-NeXT-Video should be done in processing. Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None None {} dict_keys(['pixel_values_videos'])\n",
      "dict_keys(['input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "inputs = processor([prompt, prompt_2], videos=[clip_baby, clip_karate], padding=True, return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None None {} dict_keys(['pixel_values_videos'])\n",
      "dict_keys(['input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "inputs = processor([prompt], videos=[clip_baby], padding=True, return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,  3148,  1001, 29901, 29871, 32000,    13, 11008,   338,   445,\n",
       "          4863,  2090,  1460, 29973,   319,  1799,  9047, 13566, 29901]],\n",
       "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "       device='cuda:0'), 'pixel_values_videos': tensor([[[[[ 9.9604e-01,  9.9604e-01,  9.9604e-01,  ...,  3.8290e-01,\n",
       "             3.8290e-01,  3.8290e-01],\n",
       "           [ 1.0106e+00,  1.0106e+00,  1.0106e+00,  ...,  3.8290e-01,\n",
       "             3.8290e-01,  3.8290e-01],\n",
       "           [ 1.0106e+00,  1.0106e+00,  1.0106e+00,  ...,  3.8290e-01,\n",
       "             3.8290e-01,  3.8290e-01],\n",
       "           ...,\n",
       "           [-1.6901e+00, -1.6901e+00, -1.6901e+00,  ..., -1.7923e+00,\n",
       "            -1.7923e+00, -1.7923e+00],\n",
       "           [-1.6901e+00, -1.6901e+00, -1.6901e+00,  ..., -1.7923e+00,\n",
       "            -1.7923e+00, -1.7923e+00],\n",
       "           [-1.6901e+00, -1.6901e+00, -1.6901e+00,  ..., -1.7923e+00,\n",
       "            -1.7923e+00, -1.7923e+00]],\n",
       "\n",
       "          [[ 1.1444e+00,  1.1444e+00,  1.1444e+00,  ...,  5.5910e-01,\n",
       "             5.5910e-01,  5.5910e-01],\n",
       "           [ 1.1594e+00,  1.1594e+00,  1.1594e+00,  ...,  5.5910e-01,\n",
       "             5.5910e-01,  5.5910e-01],\n",
       "           [ 1.1594e+00,  1.1594e+00,  1.1594e+00,  ...,  5.5910e-01,\n",
       "             5.5910e-01,  5.5910e-01],\n",
       "           ...,\n",
       "           [-1.5570e+00, -1.5570e+00, -1.5570e+00,  ..., -1.6921e+00,\n",
       "            -1.6921e+00, -1.6921e+00],\n",
       "           [-1.5570e+00, -1.5570e+00, -1.5570e+00,  ..., -1.6921e+00,\n",
       "            -1.6921e+00, -1.6921e+00],\n",
       "           [-1.5570e+00, -1.5570e+00, -1.5570e+00,  ..., -1.6921e+00,\n",
       "            -1.6921e+00, -1.6921e+00]],\n",
       "\n",
       "          [[ 1.2643e+00,  1.2643e+00,  1.2643e+00,  ...,  7.9499e-01,\n",
       "             7.9499e-01,  7.9499e-01],\n",
       "           [ 1.2785e+00,  1.2785e+00,  1.2785e+00,  ...,  7.9499e-01,\n",
       "             7.9499e-01,  7.9499e-01],\n",
       "           [ 1.2785e+00,  1.2785e+00,  1.2785e+00,  ...,  7.9499e-01,\n",
       "             7.9499e-01,  7.9499e-01],\n",
       "           ...,\n",
       "           [-1.2527e+00, -1.2527e+00, -1.2527e+00,  ..., -1.3807e+00,\n",
       "            -1.3807e+00, -1.3807e+00],\n",
       "           [-1.2527e+00, -1.2527e+00, -1.2527e+00,  ..., -1.3807e+00,\n",
       "            -1.3807e+00, -1.3807e+00],\n",
       "           [-1.2527e+00, -1.2527e+00, -1.2527e+00,  ..., -1.3807e+00,\n",
       "            -1.3807e+00, -1.3807e+00]]],\n",
       "\n",
       "\n",
       "         [[[ 1.7853e-01,  1.9312e-01,  2.0772e-01,  ...,  7.0407e-01,\n",
       "             7.0407e-01,  7.0407e-01],\n",
       "           [ 1.7853e-01,  1.9312e-01,  2.0772e-01,  ...,  7.0407e-01,\n",
       "             7.0407e-01,  7.0407e-01],\n",
       "           [ 1.7853e-01,  1.9312e-01,  2.0772e-01,  ...,  6.7487e-01,\n",
       "             6.7487e-01,  6.7487e-01],\n",
       "           ...,\n",
       "           [-1.6755e+00, -1.6901e+00, -1.7047e+00,  ..., -1.6463e+00,\n",
       "            -1.6609e+00, -1.6901e+00],\n",
       "           [-1.6901e+00, -1.7047e+00, -1.7047e+00,  ..., -1.6755e+00,\n",
       "            -1.6901e+00, -1.7047e+00],\n",
       "           [-1.6901e+00, -1.7047e+00, -1.7193e+00,  ..., -1.6755e+00,\n",
       "            -1.6901e+00, -1.7047e+00]],\n",
       "\n",
       "          [[ 4.3904e-01,  4.5404e-01,  4.3904e-01,  ...,  9.4930e-01,\n",
       "             9.4930e-01,  9.4930e-01],\n",
       "           [ 4.3904e-01,  4.5404e-01,  4.3904e-01,  ...,  9.4930e-01,\n",
       "             9.4930e-01,  9.4930e-01],\n",
       "           [ 4.3904e-01,  4.5404e-01,  4.3904e-01,  ...,  9.1929e-01,\n",
       "             9.1929e-01,  9.1929e-01],\n",
       "           ...,\n",
       "           [-1.5120e+00, -1.5270e+00, -1.5420e+00,  ..., -1.5270e+00,\n",
       "            -1.5420e+00, -1.5720e+00],\n",
       "           [-1.5270e+00, -1.5420e+00, -1.5420e+00,  ..., -1.5570e+00,\n",
       "            -1.5720e+00, -1.5870e+00],\n",
       "           [-1.5270e+00, -1.5420e+00, -1.5570e+00,  ..., -1.5570e+00,\n",
       "            -1.5720e+00, -1.5870e+00]],\n",
       "\n",
       "          [[ 6.1013e-01,  6.1013e-01,  5.9591e-01,  ...,  1.1078e+00,\n",
       "             1.1078e+00,  1.1078e+00],\n",
       "           [ 6.1013e-01,  6.1013e-01,  5.9591e-01,  ...,  1.1078e+00,\n",
       "             1.1078e+00,  1.1078e+00],\n",
       "           [ 6.1013e-01,  6.1013e-01,  5.9591e-01,  ...,  1.0794e+00,\n",
       "             1.0794e+00,  1.0794e+00],\n",
       "           ...,\n",
       "           [-1.1816e+00, -1.1958e+00, -1.2100e+00,  ..., -1.1816e+00,\n",
       "            -1.1958e+00, -1.2243e+00],\n",
       "           [-1.1958e+00, -1.2100e+00, -1.2100e+00,  ..., -1.2100e+00,\n",
       "            -1.2243e+00, -1.2385e+00],\n",
       "           [-1.1958e+00, -1.2100e+00, -1.2243e+00,  ..., -1.2100e+00,\n",
       "            -1.2243e+00, -1.2385e+00]]],\n",
       "\n",
       "\n",
       "         [[[ 3.2541e-02,  3.2541e-02,  3.2541e-02,  ...,  1.3473e-01,\n",
       "             1.3473e-01,  1.3473e-01],\n",
       "           [ 3.2541e-02,  3.2541e-02,  3.2541e-02,  ...,  1.3473e-01,\n",
       "             1.3473e-01,  1.3473e-01],\n",
       "           [ 3.2541e-02,  3.2541e-02,  3.2541e-02,  ...,  1.3473e-01,\n",
       "             1.3473e-01,  1.3473e-01],\n",
       "           ...,\n",
       "           [-4.2001e-01, -4.2001e-01, -4.2001e-01,  ..., -2.0103e-01,\n",
       "            -2.0103e-01, -2.0103e-01],\n",
       "           [-4.2001e-01, -4.2001e-01, -4.2001e-01,  ..., -2.3023e-01,\n",
       "            -2.3023e-01, -2.0103e-01],\n",
       "           [-4.2001e-01, -4.2001e-01, -4.2001e-01,  ..., -2.3023e-01,\n",
       "            -2.3023e-01, -2.0103e-01]],\n",
       "\n",
       "          [[ 2.4394e-01,  2.4394e-01,  2.4394e-01,  ...,  3.0397e-01,\n",
       "             3.0397e-01,  3.0397e-01],\n",
       "           [ 2.4394e-01,  2.4394e-01,  2.4394e-01,  ...,  3.0397e-01,\n",
       "             3.0397e-01,  3.0397e-01],\n",
       "           [ 2.4394e-01,  2.4394e-01,  2.4394e-01,  ...,  3.0397e-01,\n",
       "             3.0397e-01,  3.0397e-01],\n",
       "           ...,\n",
       "           [-2.9634e-01, -2.9634e-01, -2.9634e-01,  ...,  1.8820e-02,\n",
       "             1.8820e-02, -1.1196e-02],\n",
       "           [-2.9634e-01, -2.9634e-01, -2.9634e-01,  ..., -1.1196e-02,\n",
       "            -1.1196e-02, -2.6204e-02],\n",
       "           [-2.9634e-01, -2.9634e-01, -2.9634e-01,  ..., -1.1196e-02,\n",
       "            -1.1196e-02, -2.6204e-02]],\n",
       "\n",
       "          [[ 4.8215e-01,  4.8215e-01,  4.8215e-01,  ...,  5.5325e-01,\n",
       "             5.5325e-01,  5.5325e-01],\n",
       "           [ 4.8215e-01,  4.8215e-01,  4.8215e-01,  ...,  5.5325e-01,\n",
       "             5.5325e-01,  5.5325e-01],\n",
       "           [ 4.8215e-01,  4.8215e-01,  4.8215e-01,  ...,  5.5325e-01,\n",
       "             5.5325e-01,  5.5325e-01],\n",
       "           ...,\n",
       "           [-1.3329e-03, -1.3329e-03, -1.3329e-03,  ...,  2.2619e-01,\n",
       "             2.2619e-01,  2.1197e-01],\n",
       "           [-1.3329e-03, -1.3329e-03, -1.3329e-03,  ...,  1.9775e-01,\n",
       "             1.9775e-01,  1.9775e-01],\n",
       "           [-1.3329e-03, -1.3329e-03, -1.3329e-03,  ...,  1.9775e-01,\n",
       "             1.9775e-01,  1.9775e-01]]],\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "         [[[ 1.3473e-01,  1.2013e-01,  1.2013e-01,  ...,  4.1210e-01,\n",
       "             4.1210e-01,  4.2670e-01],\n",
       "           [ 1.3473e-01,  1.2013e-01,  1.2013e-01,  ...,  4.1210e-01,\n",
       "             4.1210e-01,  4.2670e-01],\n",
       "           [ 1.3473e-01,  1.2013e-01,  1.2013e-01,  ...,  4.1210e-01,\n",
       "             4.1210e-01,  4.2670e-01],\n",
       "           ...,\n",
       "           [ 1.6393e-01,  1.6393e-01,  1.4933e-01,  ...,  2.2232e-01,\n",
       "             2.0772e-01,  2.0772e-01],\n",
       "           [ 1.7853e-01,  1.6393e-01,  1.4933e-01,  ...,  2.2232e-01,\n",
       "             2.0772e-01,  2.0772e-01],\n",
       "           [ 1.7853e-01,  1.6393e-01,  1.4933e-01,  ...,  2.2232e-01,\n",
       "             2.0772e-01,  2.0772e-01]],\n",
       "\n",
       "          [[ 3.7901e-01,  3.6400e-01,  3.6400e-01,  ...,  6.3414e-01,\n",
       "             6.3414e-01,  6.4915e-01],\n",
       "           [ 3.7901e-01,  3.6400e-01,  3.6400e-01,  ...,  6.3414e-01,\n",
       "             6.3414e-01,  6.4915e-01],\n",
       "           [ 3.7901e-01,  3.6400e-01,  3.6400e-01,  ...,  6.3414e-01,\n",
       "             6.3414e-01,  6.4915e-01],\n",
       "           ...,\n",
       "           [ 3.4899e-01,  3.6400e-01,  3.7901e-01,  ...,  3.0397e-01,\n",
       "             2.8896e-01,  2.8896e-01],\n",
       "           [ 3.4899e-01,  3.6400e-01,  3.7901e-01,  ...,  3.0397e-01,\n",
       "             2.8896e-01,  2.8896e-01],\n",
       "           [ 3.4899e-01,  3.6400e-01,  3.7901e-01,  ...,  3.0397e-01,\n",
       "             2.8896e-01,  2.8896e-01]],\n",
       "\n",
       "          [[ 6.2435e-01,  6.1013e-01,  6.1013e-01,  ...,  8.8031e-01,\n",
       "             8.8031e-01,  8.6609e-01],\n",
       "           [ 6.2435e-01,  6.1013e-01,  6.1013e-01,  ...,  8.8031e-01,\n",
       "             8.8031e-01,  8.6609e-01],\n",
       "           [ 6.2435e-01,  6.1013e-01,  6.1013e-01,  ...,  8.8031e-01,\n",
       "             8.8031e-01,  8.6609e-01],\n",
       "           ...,\n",
       "           [ 5.5325e-01,  5.5325e-01,  5.6747e-01,  ...,  5.3903e-01,\n",
       "             5.2481e-01,  5.2481e-01],\n",
       "           [ 5.5325e-01,  5.6747e-01,  5.6747e-01,  ...,  5.3903e-01,\n",
       "             5.2481e-01,  5.2481e-01],\n",
       "           [ 5.5325e-01,  5.6747e-01,  5.6747e-01,  ...,  5.3903e-01,\n",
       "             5.2481e-01,  5.2481e-01]]],\n",
       "\n",
       "\n",
       "         [[[ 7.6336e-02,  7.6336e-02,  7.6336e-02,  ...,  3.9750e-01,\n",
       "             3.8290e-01,  3.8290e-01],\n",
       "           [ 9.0935e-02,  9.0935e-02,  9.0935e-02,  ...,  3.8290e-01,\n",
       "             3.8290e-01,  3.8290e-01],\n",
       "           [ 9.0935e-02,  9.0935e-02,  9.0935e-02,  ...,  4.1210e-01,\n",
       "             3.9750e-01,  3.8290e-01],\n",
       "           ...,\n",
       "           [ 2.3692e-01,  2.3692e-01,  2.5152e-01,  ...,  6.1738e-02,\n",
       "             6.1738e-02,  7.6336e-02],\n",
       "           [ 2.3692e-01,  2.2232e-01,  2.3692e-01,  ...,  6.1738e-02,\n",
       "             6.1738e-02,  7.6336e-02],\n",
       "           [ 2.3692e-01,  2.2232e-01,  2.3692e-01,  ...,  6.1738e-02,\n",
       "             6.1738e-02,  7.6336e-02]],\n",
       "\n",
       "          [[ 3.1897e-01,  3.1897e-01,  3.1897e-01,  ...,  6.1913e-01,\n",
       "             6.0412e-01,  5.7411e-01],\n",
       "           [ 3.3398e-01,  3.3398e-01,  3.3398e-01,  ...,  6.0412e-01,\n",
       "             6.0412e-01,  5.7411e-01],\n",
       "           [ 3.3398e-01,  3.3398e-01,  3.3398e-01,  ...,  5.8911e-01,\n",
       "             5.5910e-01,  5.5910e-01],\n",
       "           ...,\n",
       "           [ 4.2403e-01,  4.2403e-01,  4.0902e-01,  ...,  2.4394e-01,\n",
       "             2.4394e-01,  2.2893e-01],\n",
       "           [ 4.2403e-01,  4.0902e-01,  3.9401e-01,  ...,  2.4394e-01,\n",
       "             2.4394e-01,  2.2893e-01],\n",
       "           [ 4.2403e-01,  4.0902e-01,  3.9401e-01,  ...,  2.4394e-01,\n",
       "             2.4394e-01,  2.2893e-01]],\n",
       "\n",
       "          [[ 5.3903e-01,  5.5325e-01,  5.6747e-01,  ...,  8.3765e-01,\n",
       "             8.2343e-01,  8.0921e-01],\n",
       "           [ 5.5325e-01,  5.6747e-01,  5.8169e-01,  ...,  8.2343e-01,\n",
       "             8.2343e-01,  8.0921e-01],\n",
       "           [ 5.5325e-01,  5.6747e-01,  5.8169e-01,  ...,  8.2343e-01,\n",
       "             8.0921e-01,  7.9499e-01],\n",
       "           ...,\n",
       "           [ 6.2435e-01,  6.2435e-01,  6.2435e-01,  ...,  4.5371e-01,\n",
       "             4.5371e-01,  4.5371e-01],\n",
       "           [ 6.2435e-01,  6.1013e-01,  6.1013e-01,  ...,  4.5371e-01,\n",
       "             4.5371e-01,  4.5371e-01],\n",
       "           [ 6.2435e-01,  6.1013e-01,  6.1013e-01,  ...,  4.5371e-01,\n",
       "             4.5371e-01,  4.5371e-01]]],\n",
       "\n",
       "\n",
       "         [[[ 5.4349e-01,  5.7268e-01,  5.8728e-01,  ...,  9.2304e-01,\n",
       "             9.2304e-01,  9.3764e-01],\n",
       "           [ 5.4349e-01,  5.7268e-01,  5.8728e-01,  ...,  9.9604e-01,\n",
       "             9.9604e-01,  1.0106e+00],\n",
       "           [ 5.4349e-01,  5.7268e-01,  5.8728e-01,  ...,  1.0106e+00,\n",
       "             1.0106e+00,  1.0252e+00],\n",
       "           ...,\n",
       "           [-1.2804e-01, -1.2804e-01, -1.2804e-01,  ..., -9.8845e-02,\n",
       "            -1.2804e-01, -1.2804e-01],\n",
       "           [-1.2804e-01, -1.2804e-01, -1.2804e-01,  ..., -1.2804e-01,\n",
       "            -1.2804e-01, -1.4264e-01],\n",
       "           [-1.2804e-01, -1.2804e-01, -1.2804e-01,  ..., -1.2804e-01,\n",
       "            -1.4264e-01, -1.4264e-01]],\n",
       "\n",
       "          [[ 7.8422e-01,  8.1423e-01,  8.2924e-01,  ...,  1.0694e+00,\n",
       "             1.0694e+00,  1.0844e+00],\n",
       "           [ 7.8422e-01,  8.1423e-01,  8.2924e-01,  ...,  1.1444e+00,\n",
       "             1.1444e+00,  1.1594e+00],\n",
       "           [ 7.8422e-01,  8.1423e-01,  8.2924e-01,  ...,  1.1594e+00,\n",
       "             1.1594e+00,  1.1744e+00],\n",
       "           ...,\n",
       "           [ 9.3858e-02,  9.3858e-02,  9.3858e-02,  ...,  7.8851e-02,\n",
       "             4.8835e-02,  4.8835e-02],\n",
       "           [ 9.3858e-02,  9.3858e-02,  9.3858e-02,  ...,  4.8835e-02,\n",
       "             4.8835e-02,  3.3827e-02],\n",
       "           [ 9.3858e-02,  9.3858e-02,  9.3858e-02,  ...,  4.8835e-02,\n",
       "             3.3827e-02,  3.3827e-02]],\n",
       "\n",
       "          [[ 9.5141e-01,  9.7985e-01,  9.9407e-01,  ...,  1.1932e+00,\n",
       "             1.1932e+00,  1.2074e+00],\n",
       "           [ 9.5141e-01,  9.7985e-01,  9.9407e-01,  ...,  1.2643e+00,\n",
       "             1.2643e+00,  1.2785e+00],\n",
       "           [ 9.5141e-01,  9.7985e-01,  9.9407e-01,  ...,  1.2785e+00,\n",
       "             1.2785e+00,  1.2927e+00],\n",
       "           ...,\n",
       "           [ 2.9729e-01,  2.9729e-01,  2.9729e-01,  ...,  2.9729e-01,\n",
       "             2.6885e-01,  2.6885e-01],\n",
       "           [ 2.9729e-01,  2.9729e-01,  2.9729e-01,  ...,  2.6885e-01,\n",
       "             2.6885e-01,  2.5463e-01],\n",
       "           [ 2.9729e-01,  2.9729e-01,  2.9729e-01,  ...,  2.6885e-01,\n",
       "             2.5463e-01,  2.5463e-01]]]]], device='cuda:0')}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 3, 336, 336])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['pixel_values_videos'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "legacy\n",
      "tensor([[    1,  3148,  1001, 29901, 29871, 32000,    13, 11008,   338,   445,\n",
      "          4863,  2090,  1460, 29973,   319,  1799,  9047, 13566, 29901]],\n",
      "       device='cuda:0')\n",
      "tensor([[    1,  3148,  1001, 29901, 29871, 32000,    13, 11008,   338,   445,\n",
      "          4863,  2090,  1460, 29973,   319,  1799,  9047, 13566, 29901]],\n",
      "       device='cuda:0')\n",
      "feature dim and lens: torch.Size([1152, 4096]) tensor([1152], device='cuda:0')\n",
      "torch.Size([1, 1170, 4096]) torch.Size([1, 1170]) torch.Size([1, 1170]) None torch.Size([1, 1170])\n",
      "input embed: torch.Size([1, 1170, 4096])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CausalLMOutputWithPast' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m generate_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_new_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m100\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_sample\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.9\u001b[39m}\n\u001b[0;32m----> 3\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mbatch_decode(output, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/envs/llasa/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/llasa/lib/python3.11/site-packages/transformers/generation/utils.py:2020\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2012\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2013\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2014\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2015\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2016\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2017\u001b[0m     )\n\u001b[1;32m   2019\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2020\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2021\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2022\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2024\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2025\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2027\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2028\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2030\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2031\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2032\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2033\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2034\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2039\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2040\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/llasa/lib/python3.11/site-packages/transformers/generation/utils.py:2966\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2963\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   2965\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2966\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2968\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2969\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/llasa/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/llasa/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/llasa/lib/python3.11/site-packages/accelerate/hooks.py:169\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/workspace/LLaSA/model/llasa_arch.py:308\u001b[0m, in \u001b[0;36mLLaSA.forward\u001b[0;34m(self, input_ids, pixel_values, pixel_values_videos, image_sizes, attention_mask, position_ids, past_key_values, inputs_embeds, vision_feature_layer, vision_feature_select_strategy, labels, use_cache, output_attentions, output_hidden_states, return_dict, segmentation_tokens)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;66;03m### segmentation addition part done \u001b[39;00m\n\u001b[1;32m    296\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlanguage_model(\n\u001b[1;32m    297\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    298\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    304\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    305\u001b[0m )\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLLM input:\u001b[39m\u001b[38;5;124m'\u001b[39m,attention_mask\u001b[38;5;241m.\u001b[39mshape,\n\u001b[1;32m    307\u001b[0m     position_ids\u001b[38;5;241m.\u001b[39mshape,\n\u001b[0;32m--> 308\u001b[0m     past_key_values,\n\u001b[1;32m    309\u001b[0m     inputs_embeds\u001b[38;5;241m.\u001b[39mshape,\n\u001b[1;32m    310\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    311\u001b[0m )\n\u001b[1;32m    315\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    317\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CausalLMOutputWithPast' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "generate_kwargs = {\"max_new_tokens\": 100, \"do_sample\": True, \"top_p\": 0.9}\n",
    "\n",
    "\n",
    "\n",
    "output = model.generate(**inputs, **generate_kwargs)\n",
    "generated_text = processor.batch_decode(output, skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"<s> USER: <video>\\nWhy is this video funny? ASSISTANT: The humor in this video comes from the unexpected and somewhat anthropomorphic interaction between a bird (seagull) and a person. In the video, a seagull takes something from a person's hand while that person is standing, which can be a surprising and amusing sight as it challenges the viewer's expectations of how humans and wildlife might interact. The seagull's boldness in snatching something from a human, possibly food, creates a\"]\n"
     ]
    }
   ],
   "source": [
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlavaNextVideoProcessor:\n",
      "- video_processor: LlavaNextVideoImageProcessor {\n",
      "  \"crop_size\": {\n",
      "    \"height\": 336,\n",
      "    \"width\": 336\n",
      "  },\n",
      "  \"do_center_crop\": true,\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_pad\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_grid_pinpoints\": [\n",
      "    [\n",
      "      336,\n",
      "      672\n",
      "    ],\n",
      "    [\n",
      "      672,\n",
      "      336\n",
      "    ],\n",
      "    [\n",
      "      672,\n",
      "      672\n",
      "    ],\n",
      "    [\n",
      "      1008,\n",
      "      336\n",
      "    ],\n",
      "    [\n",
      "      336,\n",
      "      1008\n",
      "    ]\n",
      "  ],\n",
      "  \"image_mean\": [\n",
      "    0.48145466,\n",
      "    0.4578275,\n",
      "    0.40821073\n",
      "  ],\n",
      "  \"image_processor_type\": \"LlavaNextVideoImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.26862954,\n",
      "    0.26130258,\n",
      "    0.27577711\n",
      "  ],\n",
      "  \"processor_class\": \"LlavaNextVideoProcessor\",\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"shortest_edge\": 336\n",
      "  }\n",
      "}\n",
      "\n",
      "- image_processor: LlavaNextImageProcessor {\n",
      "  \"crop_size\": {\n",
      "    \"height\": 336,\n",
      "    \"width\": 336\n",
      "  },\n",
      "  \"do_center_crop\": true,\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_pad\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_grid_pinpoints\": [\n",
      "    [\n",
      "      336,\n",
      "      672\n",
      "    ],\n",
      "    [\n",
      "      672,\n",
      "      336\n",
      "    ],\n",
      "    [\n",
      "      672,\n",
      "      672\n",
      "    ],\n",
      "    [\n",
      "      1008,\n",
      "      336\n",
      "    ],\n",
      "    [\n",
      "      336,\n",
      "      1008\n",
      "    ]\n",
      "  ],\n",
      "  \"image_mean\": [\n",
      "    0.48145466,\n",
      "    0.4578275,\n",
      "    0.40821073\n",
      "  ],\n",
      "  \"image_processor_type\": \"LlavaNextImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.26862954,\n",
      "    0.26130258,\n",
      "    0.27577711\n",
      "  ],\n",
      "  \"processor_class\": \"LlavaNextVideoProcessor\",\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"shortest_edge\": 336\n",
      "  }\n",
      "}\n",
      "\n",
      "- tokenizer: LlamaTokenizerFast(name_or_path='llava-hf/LLaVA-NeXT-Video-7B-hf', vocab_size=32000, model_max_length=4096, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32000: AddedToken(\"<video>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32001: AddedToken(\"<image>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "\n",
      "{\n",
      "  \"chat_template\": \"{% for message in messages %}{% if message['role'] != 'system' %}{{ message['role'].upper() + ': '}}{% endif %}{# Render all images first #}{% for content in message['content'] | selectattr('type', 'equalto', 'image') %}{{ '<image>\\n' }}{% endfor %}{# Render all images first #}{% for content in message['content'] | selectattr('type', 'equalto', 'video') %}{{ '<video>\\n' }}{% endfor %}{# Render all text next #}{% if message['role'] != 'assistant' %}{% for content in message['content'] | selectattr('type', 'equalto', 'text') %}{{ content['text'] + ' '}}{% endfor %}{% else %}{% for content in message['content'] | selectattr('type', 'equalto', 'text') %}{% generation %}{{ content['text'] + ' '}}{% endgeneration %}{% endfor %}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ 'ASSISTANT:' }}{% endif %}\",\n",
      "  \"image_token\": \"<image>\",\n",
      "  \"patch_size\": null,\n",
      "  \"processor_class\": \"LlavaNextVideoProcessor\",\n",
      "  \"video_token\": \"<video>\",\n",
      "  \"vision_feature_select_strategy\": null\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlavaNextVideoConfig {\n",
      "  \"_name_or_path\": \"llava-hf/LLaVA-NeXT-Video-7B-hf\",\n",
      "  \"architectures\": [\n",
      "    \"LlavaNextVideoForConditionalGeneration\"\n",
      "  ],\n",
      "  \"ignore_index\": -100,\n",
      "  \"image_grid_pinpoints\": [\n",
      "    [\n",
      "      336,\n",
      "      672\n",
      "    ],\n",
      "    [\n",
      "      672,\n",
      "      336\n",
      "    ],\n",
      "    [\n",
      "      672,\n",
      "      672\n",
      "    ],\n",
      "    [\n",
      "      1008,\n",
      "      336\n",
      "    ],\n",
      "    [\n",
      "      336,\n",
      "      1008\n",
      "    ]\n",
      "  ],\n",
      "  \"image_seq_length\": 576,\n",
      "  \"image_token_index\": 32001,\n",
      "  \"model_type\": \"llava_next_video\",\n",
      "  \"projector_hidden_act\": \"gelu\",\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"float16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"fp4\",\n",
      "    \"bnb_4bit_use_double_quant\": false,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"spatial_pool_mode\": \"average\",\n",
      "  \"spatial_pool_out_channels\": 1024,\n",
      "  \"spatial_pool_stride\": 2,\n",
      "  \"text_config\": {\n",
      "    \"_name_or_path\": \"lmsys/vicuna-7b-v1.5\",\n",
      "    \"add_cross_attention\": false,\n",
      "    \"architectures\": [\n",
      "      \"LlamaForCausalLM\"\n",
      "    ],\n",
      "    \"attention_bias\": false,\n",
      "    \"attention_dropout\": 0.0,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"begin_suppress_tokens\": null,\n",
      "    \"bos_token_id\": 1,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"cross_attention_hidden_size\": null,\n",
      "    \"decoder_start_token_id\": null,\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"early_stopping\": false,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"eos_token_id\": 2,\n",
      "    \"exponential_decay_length_penalty\": null,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"head_dim\": 128,\n",
      "    \"hidden_act\": \"silu\",\n",
      "    \"hidden_size\": 4096,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 11008,\n",
      "    \"is_decoder\": false,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"max_length\": 20,\n",
      "    \"max_position_embeddings\": 4096,\n",
      "    \"min_length\": 0,\n",
      "    \"mlp_bias\": false,\n",
      "    \"model_type\": \"llama\",\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_attention_heads\": 32,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_hidden_layers\": 32,\n",
      "    \"num_key_value_heads\": 32,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": 0,\n",
      "    \"prefix\": null,\n",
      "    \"pretraining_tp\": 1,\n",
      "    \"problem_type\": null,\n",
      "    \"pruned_heads\": {},\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"return_dict\": true,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"rms_norm_eps\": 1e-05,\n",
      "    \"rope_scaling\": {\n",
      "      \"factor\": 2.5,\n",
      "      \"rope_type\": \"linear\",\n",
      "      \"type\": \"linear\"\n",
      "    },\n",
      "    \"rope_theta\": 10000.0,\n",
      "    \"sep_token_id\": null,\n",
      "    \"suppress_tokens\": null,\n",
      "    \"task_specific_params\": null,\n",
      "    \"temperature\": 1.0,\n",
      "    \"tf_legacy_loss\": false,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_word_embeddings\": false,\n",
      "    \"tokenizer_class\": null,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": \"float16\",\n",
      "    \"torchscript\": false,\n",
      "    \"type\": \"linear\",\n",
      "    \"typical_p\": 1.0,\n",
      "    \"use_bfloat16\": false,\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 32064\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.45.0.dev0\",\n",
      "  \"use_image_newline_parameter\": true,\n",
      "  \"video_seq_length\": 288,\n",
      "  \"video_token_index\": 32000,\n",
      "  \"vision_config\": {\n",
      "    \"_name_or_path\": \"\",\n",
      "    \"add_cross_attention\": false,\n",
      "    \"architectures\": null,\n",
      "    \"attention_dropout\": 0.0,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"begin_suppress_tokens\": null,\n",
      "    \"bos_token_id\": null,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"cross_attention_hidden_size\": null,\n",
      "    \"decoder_start_token_id\": null,\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"early_stopping\": false,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"eos_token_id\": null,\n",
      "    \"exponential_decay_length_penalty\": null,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"hidden_act\": \"quick_gelu\",\n",
      "    \"hidden_size\": 1024,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"image_size\": 336,\n",
      "    \"initializer_factor\": 1.0,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 4096,\n",
      "    \"is_decoder\": false,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"layer_norm_eps\": 1e-05,\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"max_length\": 20,\n",
      "    \"min_length\": 0,\n",
      "    \"model_type\": \"clip_vision_model\",\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_attention_heads\": 16,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_channels\": 3,\n",
      "    \"num_hidden_layers\": 24,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": null,\n",
      "    \"patch_size\": 14,\n",
      "    \"prefix\": null,\n",
      "    \"problem_type\": null,\n",
      "    \"projection_dim\": 768,\n",
      "    \"pruned_heads\": {},\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"return_dict\": true,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"sep_token_id\": null,\n",
      "    \"suppress_tokens\": null,\n",
      "    \"task_specific_params\": null,\n",
      "    \"temperature\": 1.0,\n",
      "    \"tf_legacy_loss\": false,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"tokenizer_class\": null,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": null,\n",
      "    \"torchscript\": false,\n",
      "    \"typical_p\": 1.0,\n",
      "    \"use_bfloat16\": false,\n",
      "    \"vocab_size\": 32000\n",
      "  },\n",
      "  \"vision_feature_layer\": -2,\n",
      "  \"vision_feature_select_strategy\": \"default\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# Load the .mat file\n",
    "data = loadmat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Open the .mat file\n",
    "with h5py.File('/workspace/LLaSA/dataset/A2D/a2d_annotation_with_instances/ouFmCnCh1wk/00030.h5', 'r') as f:\n",
    "    # List all variables stored in the .mat file\n",
    "    print(\"Keys in the file:\", list(f.keys()))\n",
    "\n",
    "\n",
    "    def print_structure(name, obj):\n",
    "        \"\"\"Print the structure of the HDF5 file\"\"\"\n",
    "        if isinstance(obj, h5py.Dataset):\n",
    "            print(f\"Dataset: {name}, Shape: {obj.shape}, Data type: {obj.dtype}\")\n",
    "        elif isinstance(obj, h5py.Group):\n",
    "            print(f\"Group: {name}\")\n",
    "\n",
    "    # Visit each object in the file\n",
    "    f.visititems(print_structure)\n",
    "\n",
    "    coord = f['reBBox'][:]\n",
    "    print(f['instance'][:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall opencv-contrib-python -y\n",
    "!pip uninstall opencv-python-headless -y \n",
    "!pip uninstall opencv-python -y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --no-cache-dir opencv-contrib-python \n",
    "!pip install  --no-cache-dir opencv-python\n",
    "!pip install --no-cache-dir opencv-python-headless\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def draw_bounding_box(image_path, bbox):\n",
    "    \"\"\"\n",
    "    Retrieve an image from a local path and draw a bounding box on it using matplotlib.\n",
    "\n",
    "    :param image_path: Path to the local image file\n",
    "    :param bbox: Bounding box coordinates in the format (x_min, y_min, x_max, y_max)\n",
    "    \"\"\"\n",
    "    # Load the image using PIL\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Create a figure and axis\n",
    "    fig, ax = plt.subplots(1)\n",
    "\n",
    "    # Display the image\n",
    "    ax.imshow(image)\n",
    "\n",
    "    # Extract bounding box coordinates\n",
    "    x_min, y_min, x_max, y_max = bbox\n",
    "\n",
    "    # Create a Rectangle patch\n",
    "    rect = patches.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min,\n",
    "                             linewidth=2, edgecolor='g', facecolor='none')\n",
    "\n",
    "    # Add the rectangle to the plot\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "image_path = '/workspace/LLaSA/dataset/A2D/clips320jpeg/ouFmCnCh1wk/00030.jpeg'  # Replace with the path to your image file\n",
    "num = 2\n",
    "bbox = tuple(sub_coord[num] for sub_coord in coord)  # Example bounding box coordinates (x_min, y_min, x_max, y_max)\n",
    "\n",
    "draw_bounding_box(image_path, bbox)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[112.7125      78.1         51.475      177.5        243.175\n",
    "  313.2875     135.7875    ]\n",
    " [136.88888889 100.44444444 158.22222222 159.11111111 110.22222222\n",
    "   88.         156.44444444]\n",
    " [165.9625      98.5125      79.875      234.3        265.3625\n",
    "  326.6        146.4375    ]\n",
    " [205.33333333 164.44444444 251.55555556 240.88888889 168.\n",
    "  104.         164.44444444]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llasa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
